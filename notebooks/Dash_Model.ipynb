{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our preamble cell :\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from category_encoders import OrdinalEncoder\n",
    "\n",
    "from sklearn import cluster\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# from shap import TreeExplainer, initjs, force_plot\n",
    "from pdpbox.pdp import pdp_interact, pdp_interact_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Real data shape:  (21417, 5)\n",
      "Fake data shape:  (23436, 5)\n"
     ]
    }
   ],
   "source": [
    "# importing and minor cleaning first, parsing dates. \n",
    "\n",
    "dfreal = pd.read_csv('True.csv',\n",
    "                    parse_dates = ['date'])\n",
    "#                    index_col = 'date')\n",
    "dfreal['Fake'] = 0\n",
    "print('Real data shape: ', dfreal.shape)\n",
    "\n",
    "dffake = pd.read_csv('Fake.csv',\n",
    "                    parse_dates = ['date'])\n",
    "#                    index_col = 'date')\n",
    "\n",
    "dffake['Fake'] = 1\n",
    "\n",
    "# I have added the boolean column for Fake/Real to each. \n",
    "\n",
    "# This next bit will clean up the messy date columns from the fake csv. \n",
    "\n",
    "searchfor = ['http', '-', 'MSNBC']\n",
    "dffake = dffake[~dffake['date'].str.contains('|'.join(searchfor))]\n",
    "\n",
    "print('Fake data shape: ', dffake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['text', 'Fake'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Now, I want to trim them to be the same size, so that we have our baseline right at 0.5 -the same as flipping a coin. \n",
    "\n",
    "dfreal_trimmed = dfreal[-21_400 :]\n",
    "\n",
    "\n",
    "dffake_trimmed = dffake[-21_400 :]\n",
    "\n",
    "\n",
    "# and now combine them into one dataframe:\n",
    "df_joined = dfreal_trimmed.append(dffake_trimmed, ignore_index=True)\n",
    "\n",
    "df_joined['date'] = pd.to_datetime(df_joined['date'])\n",
    "df_joined.drop(['subject'], axis=1, inplace=True)\n",
    "df_joined.drop(['date'], axis=1, inplace=True)\n",
    "\n",
    "df_joined = df_joined.drop(['title'], axis=1)\n",
    "\n",
    "print(df_joined.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(27392,)\n(27392,)\n(6848,)\n(6848,)\n(8560,)\n(8560,)\n"
     ]
    }
   ],
   "source": [
    "target_body = df_joined['Fake']\n",
    "Xb = df_joined['text']\n",
    "yb = target_body\n",
    "\n",
    "\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(Xb, yb, test_size=0.2, random_state=42)\n",
    "\n",
    "# doing 20/80 split and 42. and then the same to split val set from train set.\n",
    "\n",
    "Xb_train, Xb_val, yb_train, yb_val = train_test_split(Xb_train, yb_train, test_size=0.2, random_state=42) \n",
    "\n",
    "print(Xb_train.shape)\n",
    "print(yb_train.shape)\n",
    "print(Xb_val.shape)\n",
    "print(yb_val.shape)\n",
    "print(Xb_test.shape)\n",
    "print(yb_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train:  0.9325350467289719\n",
      "val:  0.9192464953271028\n",
      "test:  0.9184579439252336\n"
     ]
    }
   ],
   "source": [
    "model_dash_TFIDF_5_50 = Pipeline([\n",
    "    ('vectorizer',TfidfVectorizer(stop_words = 'english', strip_accents ='ascii', max_features = 100, min_df= 0.05 , max_df= 0.5)),\n",
    "    #('dim_red', TruncatedSVD(n_components=19, random_state=42)),\n",
    "    ('encoder', OrdinalEncoder()),\n",
    "    ('predictor', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "model_dash_TFIDF_5_50.fit(Xb_train, yb_train);\n",
    "print('train: ', model_dash_TFIDF_5_50.score(Xb_train, yb_train))\n",
    "print('val: ', model_dash_TFIDF_5_50.score(Xb_val, yb_val))\n",
    "print('test: ',model_dash_TFIDF_5_50.score(Xb_test, yb_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}